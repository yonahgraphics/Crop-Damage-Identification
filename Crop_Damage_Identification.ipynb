{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T6jpI2vM5sT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Load the train data\n",
        "train_df = pd.read_csv('Train.csv')\n",
        "\n",
        "# Define image paths and labels\n",
        "image_dir = 'train'  # Directory containing training images\n",
        "train_df['./images'] = train_df['ID'].apply(lambda x: os.path.join(image_dir, x + '.jpg'))\n",
        "\n",
        "# Load images and resize them\n",
        "def load_image(img_path, img_size):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (img_size, img_size))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img / 255.0  # Normalize\n",
        "    return img\n",
        "\n",
        "img_size = 224  # Resize images to 224x224 for EfficientNet\n",
        "X = np.array([load_image(img_path, img_size) for img_path in train_df['image_path']])\n",
        "y = pd.get_dummies(train_df['Damage']).values  # One-hot encode the labels\n",
        "\n",
        "# Split into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator()\n",
        "\n",
        "# Pretrained model (EfficientNet)\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(y_train.shape[1], activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks for early stopping and model checkpointing\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=5, restore_best_weights=True),\n",
        "    ModelCheckpoint('best_model.h5', save_best_only=True)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_datagen.flow(X_train, y_train, batch_size=32),\n",
        "    validation_data=val_datagen.flow(X_val, y_val),\n",
        "    epochs=20,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Load the test data\n",
        "test_df = pd.read_csv('Test.csv')\n",
        "test_df['image_path'] = test_df['ID'].apply(lambda x: os.path.join('test', x + '.jpg'))\n",
        "X_test = np.array([load_image(img_path, img_size) for img_path in test_df['image_path']])\n",
        "\n",
        "# Predict on the test set\n",
        "predictions = model.predict(X_test)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Map predicted labels back to the original labels\n",
        "label_mapping = {i: label for i, label in enumerate(train_df['Damage'].unique())}\n",
        "test_df['Damage'] = predicted_labels.map(label_mapping)\n",
        "\n",
        "# Prepare the submission file\n",
        "submission_df = test_df[['ID', 'Damage']]\n",
        "submission_df.to_csv('submission.csv', index=False)\n"
      ]
    }
  ]
}